{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Supervised Machine Learning\n",
    "### Artificial Intelligence 1, Week 6\n",
    "\n",
    "\n",
    "### Learning models for **classification** or **regression** from a set of labelled instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# This week\n",
    "Learning outcomes:\n",
    "\n",
    "- Identify formulate and apply the basic processes of supervised machine learning\n",
    "- Understand the role of data in estimating accuracy \n",
    "\n",
    "Videos:\n",
    "- Basic model building process: train and test \n",
    "- Types of model: instance-based ( e.g. kNN) vs explicit (e.g. decision trees,rules, ...) \n",
    "- Example:   greedy rule induction as compared to expert system\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning Paradigm\n",
    "- Completely different paradigm to symbolic AI\n",
    "- Create a system with the ability to learn\n",
    "- Present the system with series of examples\n",
    "- System builds up its own model of the world\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"figures/PersonThinkingAboutDogs.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"figures/idealisedDog.png\" style=\"float:right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Video (6:52): Hello World of Machine Learning Recipes\n",
    "\n",
    "\n",
    "https://youtu.be/cKxRvEZd3Mw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## It's all about the data\n",
    "- Computers cannot experience artefacts of the real world directly\n",
    "- Instead they just deal with a few variables that represent them\n",
    "- ML algorithms learn from a “training set” containing digital representations of examples to learn from\n",
    "- Outcomes depend entirely on:\n",
    " - What you choose to measure\n",
    " - And how representative your training set is\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## More formally\n",
    "\n",
    "We have a set of *n* examples., and for each one  we have: \n",
    "- a value for each of *f* features \n",
    "- a label\n",
    "\n",
    "The data set *X* is usually 2-D array of *n* rows and *f* columns.   \n",
    "The label set *y* is usually a 1-D array with *n* entries.   \n",
    "For now we'll assume the features are *continous* (e.g. floating point values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If the label comes from a discrete unordered set of *m* values, e.g.  (\"Orange\",\"Apple\" \"Banana\"): \n",
    "- we have a **Classification** problem.  \n",
    "- We learn a model *M* that is a mapping from a *f*-dimensional continuous space (the feature values) onto a finite set\n",
    "- *M*: R<sup>f</sup> --> \\{1,...,m\\}\n",
    "\n",
    "If the label is an ordinal value (integer,    floating point):\n",
    "- we have a **Regression** problem.\n",
    "- *M*:R<sup>f</sup>->R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The  Supervised Learning Workflow\n",
    "\n",
    "<img src=\"figures/ML_workflow.png\" style= \"float:right\">\n",
    "\n",
    "This diagram assumes you are trying out more than one type of algorithm or choice of parameter settings\n",
    "\n",
    "If you are just trying one algorithm you can skip the validation phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example:  Iris flowers <img src=\"figures/Iris-image.png\" style=\"float:right\">\n",
    "- Classic Machine Learning Data set\n",
    "- 4 measurements: sepal and petal width and length\n",
    "- 50 examples  from each 3 sub-species for iris flowers\n",
    "- three class problem:\n",
    " - so for some types of algorithm have to decide whether to make  \n",
    "   a 3-way classifier or nested 1-vs-rest classifers\n",
    "- most ML classifiers can get over 90%\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "ExecuteTime": {
     "start_time": "2024-04-20T04:16:57.619039Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets\n",
    "import  week7_utils as W7utils\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "irisX,irisy = sklearn.datasets.load_iris(return_X_y=True)\n",
    "iris_features= (\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\")\n",
    "iris_names= ['setosa','versicolor','virginica']\n",
    "title=\"Scatterplots of 2D slices through the 4D Iris data\"\n",
    "W7utils.show_scatterplot_matrix(irisX,irisy,iris_features,title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recap so far\n",
    "Machine Learning is about learning patterns from data. In supervised ML this means:\n",
    "\n",
    "**Training Data**: set of labelled examples, each characterised by values for *f* features  \n",
    "**X**: data - usually a 2D array with one row per example, one column for each feature  \n",
    "  (even images can be 'flattened' into this format).   \n",
    "**y** : the labels/target \n",
    "\n",
    "A supervised Machine Learning **Algorithm**\n",
    "\n",
    "A **performance criteria**: used to drive training and then estimate quality of model.  \n",
    "Depending on the **context** this might be accuracy,  precision, recall,...\n",
    "\n",
    "\n",
    "A **test set** to estimate the performance of the model on unseen data.  \n",
    "If this is not available separately, have to take out some data from the training set\n",
    " - crude way; single 70:30 train:test split, making sure you preserve the proportions of different classes\n",
    " - better way: split data into ten\n",
    "   - repeatedly train on 9/10 test on remaining 1/10, \n",
    "   - \"headline\" result is mean, but keep split results for statistical testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Important Idea!  Decision Surfaces\n",
    "<img src=\"figures/decisionRegions.png\" style=\"float:right\" width=40%>\n",
    "\n",
    "Each feature defines a dimension in *feature space*.\n",
    "\n",
    "Each example has specific values for each feature\n",
    "- so it occupies one point in feature space\n",
    "\n",
    "The aim of our model is to let us predict labels for any item\n",
    "- so it puts decision boundaries into that space to  \n",
    "  divide it into regions\n",
    "\n",
    "Symbolic Reasoning: \n",
    "- boundaries defined by our 'knowledge' \n",
    "- so can plot without needing data!\n",
    "\n",
    "Machine Learning: \n",
    "- use the training data to **estimate** where the boundaries should be\n",
    "- then plots model's prediction for lots of points over a grid  \n",
    "  to find the decision surface and boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Machine Learning Algorithms\n",
    "Typically a ML method consists of:\n",
    "\n",
    "1: A  representation for the decision boundaries\n",
    " - Each different arrangement of boundaries defines a unique model\n",
    " - Each unique model is defined by the set of values for variables specifying where the boundaries are.\n",
    " - Different types of models will have different variables.\n",
    " \n",
    "2: A learning algorithm to deciding how to change those variable values to move between models\n",
    " - last week we saw how the KMeans clustering algorirthm uses \"local search with random restarts\"\n",
    "\n",
    "ML Algorithms build models in different ways\n",
    "- but they don’t care what it is they are grouping\n",
    "- and it is **meaningless** to say they “understand”.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Some example ML methods\n",
    "The field of ML is fast growing and contains many complex methods and representations\n",
    "In this module I will just focus on a few simple ideas to give you a feel for what is out there.  \n",
    "- Instance-based learning (k-Nearest Neighbours) - this week\n",
    "- Decision trees and rule induction algorithms- this week\n",
    "- Artificial Neural Networks - weeks 7 and 8  //// this was week 7 and now hthese will be wrong \n",
    "\n",
    "Next year: \n",
    "- Artificial Intelligence 2:  15 credits, semester 1 (AI and \"General\" pathways)\n",
    "and in particular ////missing end of semntance or accidentally copied in half sentance////\n",
    "- Machine Learning: 15 credits, semester 2     ( AI pathway)\n",
    "\n",
    "will cover more algorithms in greater depth.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Instance-based Methods: Nearest Neighbour Methods\n",
    "- Do not explicitly represent class boundaries  \n",
    "  Construct them “on-the-fly” when queried\n",
    "- Store the set of training examples  \n",
    "  More efficient methods may not store all points\n",
    "- Use a metric to calculate distance between two points  \n",
    "  e.g. Euclidean (continuous), Hamming (binary), ...\n",
    "\n",
    "<img src=\"figures/kNN-steps.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## K-Nearest Neighbour Classification \n",
    "<img src=\"figures/voronoi.png\" style=\"float:right\" width = 400 title=\"https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor\">\n",
    "\n",
    "**init(neighbours=k, distance metric =d)**  :  \n",
    "Specify k and a distance metric d(i,j) \n",
    "\n",
    "**fit(trainingData)** :  \n",
    "Store a local copy of the training data as two arrays:  \n",
    "model_X of shape (numTrainingItems , numFeatures),  \n",
    "model_y of shape( numTrainingItems)\n",
    "  \n",
    "**predict(newItems)** :  \n",
    "*Step 1:   Make 2D array **distances** of shape (num_newItems , numTrainingItems)*   \n",
    "FOREACH COMBINATION of newItem i  and trainingitem j  \n",
    "...SET **distances [i] [j]** = d (i,j) \n",
    "\n",
    "*Step 2: Make 2D array **votes** of shape(num_newItems, k)*  \n",
    "FOREACH newItem i  \n",
    "...Find the *k* columns of the row **distances[i]** with the smallest values  \n",
    "...Put the corresponding *k* labels from model_y into **votes[i]**  \n",
    "\n",
    "*Step 3: Store majority vote in a  1D array y_pred of size (numToPredict)*   \n",
    "FOREACH  newItem i  \n",
    "...SET y_pred[i] = most_common_value(votes[i]) \n",
    "\n",
    "RETURN y_pred\n",
    "\n",
    "Image adapted from Vornoi tesselation for kNN from https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "ExecuteTime": {
     "start_time": "2024-04-20T04:16:57.620039Z"
    }
   },
   "outputs": [],
   "source": [
    "# Example for K = 1 \n",
    "class simple_1NN:\n",
    "\n",
    "    def fit(self,X,y):\n",
    "        self.numTrainingItems = X.shape[0]\n",
    "        self.numFeatures = X.shape[1]\n",
    "        self.model_X = X\n",
    "        self.model_y = y\n",
    "        \n",
    "    def predict(self,newItems):\n",
    "        numToPredict = newItems.shape[0]\n",
    "        yPred = np.zeros((numToPredict),dtype=int)\n",
    "        \n",
    "        # measure distances - creates an array with numToPredict rows and num_trainItems columns\n",
    "        dist = np.zeros((numToPredict,self.numTrainingItems))\n",
    "        for new_item in range(numToPredict):\n",
    "            for stored_example in range(self.numTrainingItems):\n",
    "                dist[new_item][stored_example]= W7utils.euclidean_distance(newItems[new_item],self.model_X[stored_example ])\n",
    "\n",
    "        #make predictions: This is K=1, TO DO- in your own time extend to work with K>1\n",
    "        for item in range(numToPredict):\n",
    "            closest = np.argmin(dist, axis=1) \n",
    "            yPred[item] = self.model_y [ closest[item]]\n",
    "        \n",
    "        return yPred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Iris\n",
    "\n",
    "We'll use a function from sklearn to do our train/test split here.\n",
    "\n",
    "This is handy because it shuffles the data and has options to make sure that we keep the same proportion of different classes in our training and testing data.\n",
    "\n",
    "\n",
    "            \n",
    "           \n",
    "We'll also make a **confusion matrix** to examine the predictions it makes\n",
    "rows = target labels,  columns = predicted labels\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "ExecuteTime": {
     "start_time": "2024-04-20T04:16:57.621037Z"
    }
   },
   "outputs": [],
   "source": [
    "# make train/test split \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np \n",
    "\n",
    "irisX,irisy = load_iris(return_X_y = True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(irisX, irisy, test_size=0.33,stratify=irisy)\n",
    "\n",
    "\n",
    "myKNNmodel = simple_1NN()\n",
    "myKNNmodel.fit(X_train,y_train)\n",
    "y_pred = myKNNmodel.predict(X_test)\n",
    "print(y_pred.T) #.t turns column to row so it sghows onscreen better "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## how good are these results?\n",
    "We can use a neat numpy trick to find out if the predictions are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print ( (y_test==y_pred))\n",
    "accuracy = 100* ( y_test == y_pred).sum() / y_test.shape[0]\n",
    "print(f\"Overall Accuracy = {accuracy} %\")\n",
    "\n",
    "confusionMatrix = np.zeros((3,3),int)\n",
    "for i in range(50):\n",
    "    actual = int(y_test[i])\n",
    "    predicted = int(y_pred[i])\n",
    "    confusionMatrix[actual][predicted] += 1\n",
    "print(confusionMatrix)\n",
    "\n",
    "#and here's sklearn's built-in method\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred,display_labels= iris_names )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Visualising 1-NN if we just learn from the petal features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "ExecuteTime": {
     "start_time": "2024-04-20T04:16:57.622039Z"
    }
   },
   "outputs": [],
   "source": [
    "petals = X_train[:,2:4]\n",
    "myKNNmodel.fit(petals,y_train)\n",
    "y_pred = myKNNmodel.predict(X_test[:,2:4])\n",
    "accuracy = 100* ( y_test == y_pred).sum() / y_test.shape[0]\n",
    "print(f\"Overall Accuracy in 2D = {accuracy} %\")\n",
    "W7utils.PlotDecisionSurface(petals,y_train,myKNNmodel, \"1-Nearest Neighbour on petal features\", iris_features[2:4],minZero=True,stepSize= 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Timeout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Rule Induction Algorithms\n",
    "\n",
    "## Principles\n",
    "\n",
    "### Rule Representation\n",
    "////In Topic One we looked at 'Knowledge-Based systems'  \n",
    "where **humans provided the rules** for a situation.\n",
    "<img src=\"figures/rule-representation.png\" style=\"float:right\" width=50%>//// not in this order ////\n",
    "\n",
    "\n",
    "In supervised learning we are interested in how we can make   \n",
    "**machines learn the rules** for an application.   \n",
    "- e.g. **if** feature_n > threshold **then** prediction.\n",
    "\n",
    "To do that we need to have:\n",
    "1. A representation for rules  \n",
    "2. A way of assigning \"goodness\" to (sets of) rules.\n",
    "3. A way of algorithmically generating possible rules  \n",
    "   We have fixed sets of features,operators,outputs,  \n",
    "   We can **discretize** the thresholds for each feature    \n",
    "   So we can use nested loops to create all possible rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "   \n",
    "### Rule Matching\n",
    "We say that a rule *covers* a training example (features, label) if\n",
    "- the example features meet the rule's _condition_\n",
    "- the rule's _action_ (prediction) matches the example's label.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Boundaries and Default Classes\n",
    "Most existing algorithms tend to use  rules built up of lots of axis-perpendicular decisions.   \n",
    "-  For example the (useless) rule  *If( petal_length > 0.3) THEN (\"Setosa\")*   \n",
    "  Draws a line through feature space, at right angles to the petal_length axis, crossing it at 0.3.  \n",
    "  Puts the label \"setosa\" on one side, nothing on the other\n",
    "\n",
    "- As more rules are added, the model effectively builds labelled (hyper) boxes in space.  \n",
    "  \n",
    "- Rest of 'decision space' is given with the default (majority) label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Greedy rule induction: keep choosing the next best rule\n",
    "\n",
    "We can exploit the ability to generate rules algorithmically to make a simple Machine Learning algorithm that **automatically** learns rules, using a greedy constructive hill climbing approach:  \n",
    "\n",
    "This is a **generate-and-test** approaich for search the space of all possible models, that repeatedly takes the \"next-best\" rule to create a rule-set.     \n",
    "- Note that this method can be easily out-perfomed by more sophisticted approaches.\n",
    "We start by making a copy of the training set: call this the _NotCovered_ data.  \n",
    "Then in a loop until the NotCovered data is empty, or we cannot add any rules that don't make errors:\n",
    "1. __Generate__ all the possible rules as described above\n",
    "2. __Test__ how how many examples from  the _NotCovered_ data each rule covers\n",
    "3. __Select__  the one that  covers the most un-covered examples **without making any wrong predictions**\n",
    "   - add the new rule to our model\n",
    "   - remove the examples that rule covered from the set of _NotCovered_ data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pseudocode\n",
    "While it is being trained, the model holds:\n",
    "- _ruleset_: a set of rules (initially empty)\n",
    "- _NotCovered_ : a set of training examples\n",
    "- a default class\n",
    "\n",
    "**Note that a set of rules may not cover every training example**\n",
    " - because it may not be possible without making wrong predictions\n",
    " - or we may choose to limit the number of rules for simplicity or explainability\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Main Learning loop  - equivalent of fit()\n",
    "```\n",
    "function GreedyRuleInduction:  \n",
    "    #step 0.\n",
    "    Preprocess (trainingset)  \n",
    "    SET model.ruleSet = empty ruleset\n",
    "    SET notCovered = Copy(trainingset)\n",
    "    SET default_prediction= GetMajorityClass(notCovered)\n",
    "\n",
    "    #Main loop\n",
    "    SET Improved=TRUE\n",
    "    WHILE ( Size(notCovered)>0  AND Size(model.ruleSet) < MaxRules  AND Improved) DO  \n",
    "        # set up\n",
    "        SET Improved= FALSE\n",
    "        SET bestNewRule = [] # empty rule\n",
    "        SET coveredByBest = [] # emptyset\n",
    "        \n",
    "        #loop to test new rule\n",
    "        FOR newRule in  (all_possible_rules)  # 4 nested for-loops to create the newrule\n",
    "            covered = GetExamplesCoveredBy(newRule, notCovered)\n",
    "            IF ( SIZE(covered) > SIZE(coveredByBest) ) THEN\n",
    "                SET coveredByBest= covered\n",
    "                SET bestNewRule = newRule\n",
    "                SET Improved=TRUE\n",
    "                \n",
    "        #add best rule if found\n",
    "        IF (Improved) THEN\n",
    "            SET notCovered = notCovered / coveredByBest.  #set exclusion operation\n",
    "            SET ruleset = ruleset + bestNewRule\n",
    "    RETURN ruleset\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful helper function\n",
    "```\n",
    "function GetExamplesCoveredBy(newRule, notCovered):\n",
    "    # assumes a rule is a tuple of [ feature,operator, threshold,prediction]\n",
    "    # and that an example is a tuple of [featureValues, label]\n",
    "    SET errors = FALSE\n",
    "    SET covered = emptyset\n",
    "    \n",
    "    FOR (example in notCovered) DO\n",
    "       IF MeetsConditions(example, rule) THEN\n",
    "          IF example.label EQUALS rule.prediction THEN\n",
    "              SET covered = covered + example\n",
    "          ELSE\n",
    "            SET errors = TRUE\n",
    "            BREAK\n",
    "            \n",
    "    IF (errors EQUALS TRUE)\n",
    "       return emptyset\n",
    "       \n",
    "    ELSE\n",
    "       return covered\n",
    "       \n",
    "```\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test if a rule applies to an example\n",
    "```\n",
    "function MeetsConditions(example, rule)\n",
    "   SET matches=FALSE\n",
    "   SET feature= rule.feature\n",
    "   SET operator = rule.operator\n",
    "   SET threshold= rule.threshold\n",
    "   \n",
    "   set exampleValue= example.featureValues[feature]\n",
    "   IF (operator IS Equals) THEN\n",
    "      IF (exampleValue EQUALS threshold) THEN\n",
    "           SET matches = TRUE\n",
    "   ELSEIF (operator IS LessThan) THEN\n",
    "      IF (exampleValue < threshold) THEN\n",
    "          SET matches = TRUE\n",
    "   ELSEIF (operator IS MoreThan) THEN\n",
    "      IF  (exampleValue > threshold) THEN\n",
    "           SET matches = TRUE\n",
    "\n",
    "   #could extend to <= etc\n",
    "  \n",
    "   RETURN matches\n",
    "```\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### apply learned ruleset to make prediction\n",
    "```\n",
    "function makePrediction(example,ruleset)\n",
    "    SET prediction= default_prediction\n",
    "    \n",
    "    FOR (rule in ruleset) DO\n",
    "        IF MeetsConditions( example,rule) THEN\n",
    "           SET prediction= rule.prediction\n",
    "           BREAK\n",
    "    return prediction\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pseudocode\n",
    "Model holds a set of rules and a score.  \n",
    "Score() uses ruleset in candidate solution to make predictions on training set  \n",
    " and sets model.score to -1 if any errors,  else number of correct predictions\n",
    " \n",
    "**Note that a set of rules may not cover every training example**\n",
    "# function Greedy_Rule_Induction\n",
    "    #step 0.\n",
    "    Preprocess (training_set)  \n",
    "    SET current_model = {}  # empty ruleset, \n",
    "    SET covered_data = {} #empty set\n",
    "    SET uncovered_data = training_set\n",
    "    \n",
    "    #Main loop- repeat until model contains a rule that predicts for each item\n",
    "\n",
    "    WHILE (len(uncoveed_data) >0) DO  \n",
    "        \n",
    "        SET best_new_rule = {} # empty rule\n",
    "        SET covered_by_best = {} #emptyset\n",
    "\n",
    "        FOR rule in  (all_possible_rules)  # 4 nested for-loops\n",
    "            SET covered_by_rule = get_items_covered_by_rules(rule,uncovered_data)\n",
    "            IF len (covered_by_this > len(covered_by_best) THEN\n",
    "               SET best_new_rule = rule\n",
    "               SET covered_by_best = covered_by_this\n",
    "        #addbest new erulw ot ruleset and updte items  \n",
    "        SET covered_data = covered_data + covered_by_best\n",
    "        SET uncovered_data = uncovered_data / covered-by_this  # / denotes set exclusion\n",
    "        SET current_model = current_model + best_new_rule\n",
    "\n",
    "    # RETURN current_model\n",
    "\n",
    "\n",
    "#function get_items_covered_by_rule (rule, dataset)\n",
    "\n",
    "     SET covered_items = {} # empty set\n",
    "\n",
    "     FOR item in dataset:\n",
    "        IF item.features meets rule.condition THEN\n",
    "           IF and item.label EQUALS rule.action THEN\n",
    "               SET covered_items = covered_items + item\n",
    "           ELSE\n",
    "               RETURN {} # return empty set as soon as  rule makes any wrong predictions\n",
    "     RETURN covered_items\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Flow chart for model learning\n",
    "<img src=\"figures/rule-induction-flowchart-fit-v2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Flowchart for predicting with model\n",
    "<img src=\"figures/rule-induction-flowchart-predict.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Timeout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees \n",
    "Tree-based structure can capture rules and more.\n",
    "\n",
    "Basic idea: divide input space using a set of axis-parallel lines by **\"growing\"** a tree\n",
    "\n",
    "1. Start with single node that predicts majority class label.\n",
    "2. Recursively:\n",
    " 1. measure the ///\"data purity\"  or \"information content\" /// how and what are these not explained /// of the data that arrives at that node\n",
    " 2. examine each way of splitting data  you could put into that node, and measure the information content of the left and right child nodes you would get from the split\n",
    " 4. if the  \"best\" split is above some threshold then add it and repeat\n",
    " \n",
    "**This criteria for adding nodes is different to the rule induction algorithm, and gives you different trees**\n",
    "\n",
    "**Interior nodes** are equivalent to conditions in a rule  \n",
    "**Leaf Nodes** are the outputs: \n",
    " - class labels (classification tree), or \n",
    " - equation for predicting values (regression tree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision trees for our example datasets\n",
    "using code from sklearn \n",
    "`class sklearn.tree.DecisionTreeClassifier(*, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort='deprecated', ccp_alpha=0.0)`\n",
    "\n",
    "Like all sklearn models it implements a fit() and predict() method\n",
    "\n",
    "Note the default criteria for splitting is the 'gini' indes = there are many available, this is a popular one\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> luca comment: code wants iris_features to be \"must be an instance of 'list' or None\"</h2?>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    },
    "ExecuteTime": {
     "start_time": "2024-04-20T04:16:57.623037Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn import tree\n",
    "\n",
    "fig,ax = plt.subplots(1,3,figsize=(15,5))\n",
    "fig.suptitle(\"Illustration of how Decision Trees select and insert nodes to increase data purity\")\n",
    "for depth in range (1,4):\n",
    "    DTmodel = DecisionTreeClassifier(random_state=1234, max_depth=depth,min_samples_split=2,min_samples_leaf=1)\n",
    "    DTmodel.fit(X_train,y_train)\n",
    "    _ = tree.plot_tree(DTmodel, feature_names=iris_features, class_names= iris_names,filled=True,ax=ax[depth-1])\n",
    "    ax[depth-1].set_title(\"Depth \"+str(depth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Visualising the results using just the petal features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "ExecuteTime": {
     "start_time": "2024-04-20T04:16:57.624037Z"
    }
   },
   "outputs": [],
   "source": [
    "petals = X_train[:,2:4]\n",
    "Two_D_DecisionTree = DecisionTreeClassifier(max_depth=4)\n",
    "Two_D_DecisionTree.fit(petals,y_train)\n",
    "_ = tree.plot_tree(Two_D_DecisionTree, feature_names=iris_features, class_names= iris_names,filled=True)\n",
    "\n",
    "W7utils.PlotDecisionSurface(petals,y_train,Two_D_DecisionTree, \"Decision Tree: simplified outcomes\", iris_features[2:4],stepSize=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## So how do  we learn models?\n",
    "**Construction**:  add boundaries to make models more complex\n",
    "- Add examples to kNN\n",
    "- Repeatedly add nodes to trees, splitting on new variables\n",
    "- Repeatedly add rules that classify as-yet unclassified data\n",
    "- Add nodes to an artifical neural network\n",
    " \n",
    "**Perturbation**: Move existing boundaries to change model\n",
    "- Change value of K or distance function in kNN\n",
    "- Change rule/treenode thresholds: *if  exam < 40*  &rarr; *if exam < 38*\n",
    "- Change operators in rules/ tree nodes:  *if exam < 38* &rarr; *if exam &leq; 38*\n",
    "- Change variables considered in rules/tree nodes: *if exam < 38* &rarr; *if coursework < 38*\n",
    "- Change weights in MLP, \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "Supervised Machine Learning is concerned with learning predictive models from datasets\n",
    "- Different algorithms use different representations of decision boundaries\n",
    "- Regions inside the boundaries contain **Class labels** or **(formulas leading to) continuous values** (regression)\n",
    "\n",
    "Algorithms **fit** models to data by repeatedly:\n",
    "  - making and testing small changes,  \n",
    "  - and then selecting the ones that improve accuracy on the training set\n",
    "  - until some stop criteria is met\n",
    "\n",
    "  - They do this by either adding complexity or changing the parameters of an existing model\n",
    "  - This is equivalent to moving through “model space”\n",
    "\n",
    "Once the model has been learned (fit) we leave it unchanged  \n",
    "  - and use it to **predict** the labels for new data points\n",
    "\n",
    "Next week:   Neural Networks\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
